{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\nfrom torch.utils.data import DataLoader\n","metadata":{"id":"Ynh1Pl6T1uhq","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:56.517126Z","iopub.execute_input":"2025-04-14T03:14:56.517984Z","iopub.status.idle":"2025-04-14T03:14:56.522370Z","shell.execute_reply.started":"2025-04-14T03:14:56.517943Z","shell.execute_reply":"2025-04-14T03:14:56.521517Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"- In this first cell, I imported all the necessary libraries for training and evaluating my CNN. This includes PyTorch for model building and torchvision for data handling.","metadata":{}},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n\n# Training and Testing here\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform_train)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform_test)\n\ntrainload = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\ntestload = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n\nclasses = trainset.classes\n","metadata":{"id":"4Zbr6Y332zIO","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:56.523586Z","iopub.execute_input":"2025-04-14T03:14:56.523905Z","iopub.status.idle":"2025-04-14T03:14:58.137277Z","shell.execute_reply.started":"2025-04-14T03:14:56.523888Z","shell.execute_reply":"2025-04-14T03:14:58.136729Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"- For this cell, I fetched CIFAR10 dataset and divided into training and testing sets. Also, I defined batch size of 128 here and did normalization which helps the model train faster and more stably.","metadata":{}},{"cell_type":"markdown","source":"Here I did some changes in the baseline model and updated for all the type which is listed below:\n\n- conv1: 32 filters\n- conv2: 64 filters\n- fc1: 256 fully connected neurons\n- fc2: 128 fully connected neurons","metadata":{}},{"cell_type":"code","source":"class LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n        self.fc1 = nn.Linear(64 * 5 * 5, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n","metadata":{"id":"YrAg6zfj25WI","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:58.138033Z","iopub.execute_input":"2025-04-14T03:14:58.138256Z","iopub.status.idle":"2025-04-14T03:14:58.144092Z","shell.execute_reply.started":"2025-04-14T03:14:58.138238Z","shell.execute_reply":"2025-04-14T03:14:58.143384Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"- In this cell, I defined my updated baseline LeNet-5 model. I increased the number of filters and used max pooling instead of average pooling. This setup helps extract stronger and more varied features from the input images.","metadata":{}},{"cell_type":"code","source":"class LeNet5Dropout(nn.Module):\n    def __init__(self):\n        super(LeNet5Dropout, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n        self.fc1 = nn.Linear(64 * 5 * 5, 256)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n","metadata":{"id":"xG4Ilqsg3COm","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:58.145515Z","iopub.execute_input":"2025-04-14T03:14:58.145886Z","iopub.status.idle":"2025-04-14T03:14:58.157078Z","shell.execute_reply.started":"2025-04-14T03:14:58.145861Z","shell.execute_reply":"2025-04-14T03:14:58.156422Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"- This cell defines my LeNet-5 model with a dropout layer. I applied dropout after the first fully connected layer to help prevent overfitting during training. My Dropout rate is 0.3 here.","metadata":{}},{"cell_type":"code","source":"class LeNet5BatchNorm(nn.Module):\n    def __init__(self):\n        super(LeNet5BatchNorm, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.fc1 = nn.Linear(64 * 5 * 5, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = x.view(-1, 64 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n","metadata":{"id":"uzTZ3D823E3-","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:58.157787Z","iopub.execute_input":"2025-04-14T03:14:58.158000Z","iopub.status.idle":"2025-04-14T03:14:58.175475Z","shell.execute_reply.started":"2025-04-14T03:14:58.157985Z","shell.execute_reply":"2025-04-14T03:14:58.174822Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"- Here, I created a version of LeNet-5 that uses batch normalization after each convolutional layer. This improves training stability and speeds up convergence.","metadata":{}},{"cell_type":"code","source":"def train_eval(model, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    crt = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in trainload:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = crt(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainload):.4f}\")\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testload:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    end_time = time.time()\n    print(f\"Test accuracy: {100 * correct / total:.2f}%\")\n    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n","metadata":{"id":"Zsh182st3HCc","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:58.176253Z","iopub.execute_input":"2025-04-14T03:14:58.176463Z","iopub.status.idle":"2025-04-14T03:14:58.189221Z","shell.execute_reply.started":"2025-04-14T03:14:58.176440Z","shell.execute_reply":"2025-04-14T03:14:58.188585Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"- This cell contains the training and evaluation function. It handles model training over multiple epochs, computes loss, and reports final accuracy and training time.","metadata":{}},{"cell_type":"code","source":"print(\"Baseline LeNet-5\")\nbaseline = LeNet5()\ntrain_eval(baseline)\n\nprint(\"\\nLeNet-5 with Dropout\")\ndropout = LeNet5Dropout()\ntrain_eval(dropout)\n\nprint(\"\\nLeNet-5 with BatchNorm\")\nbatchnorm = LeNet5BatchNorm()\ntrain_eval(batchnorm)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"33eLo96N3JUy","outputId":"345bf559-c265-4c78-96fd-ce96169809a7","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:14:58.189871Z","iopub.execute_input":"2025-04-14T03:14:58.190060Z","iopub.status.idle":"2025-04-14T03:18:39.308570Z","shell.execute_reply.started":"2025-04-14T03:14:58.190046Z","shell.execute_reply":"2025-04-14T03:18:39.307655Z"}},"outputs":[{"name":"stdout","text":"Baseline LeNet-5\nEpoch 1, Loss: 1.5079\nEpoch 2, Loss: 1.1066\nEpoch 3, Loss: 0.9394\nEpoch 4, Loss: 0.8157\nEpoch 5, Loss: 0.7130\nEpoch 6, Loss: 0.6284\nEpoch 7, Loss: 0.5473\nEpoch 8, Loss: 0.4771\nEpoch 9, Loss: 0.4112\nEpoch 10, Loss: 0.3387\nTest accuracy: 72.02%\nTraining time: 73.85 seconds\n\nLeNet-5 with Dropout\nEpoch 1, Loss: 1.5789\nEpoch 2, Loss: 1.2087\nEpoch 3, Loss: 1.0334\nEpoch 4, Loss: 0.9246\nEpoch 5, Loss: 0.8333\nEpoch 6, Loss: 0.7607\nEpoch 7, Loss: 0.6934\nEpoch 8, Loss: 0.6431\nEpoch 9, Loss: 0.5946\nEpoch 10, Loss: 0.5447\nTest accuracy: 72.38%\nTraining time: 73.97 seconds\n\nLeNet-5 with BatchNorm\nEpoch 1, Loss: 1.3263\nEpoch 2, Loss: 0.9804\nEpoch 3, Loss: 0.8290\nEpoch 4, Loss: 0.7209\nEpoch 5, Loss: 0.6446\nEpoch 6, Loss: 0.5696\nEpoch 7, Loss: 0.5092\nEpoch 8, Loss: 0.4532\nEpoch 9, Loss: 0.3943\nEpoch 10, Loss: 0.3533\nTest accuracy: 74.57%\nTraining time: 73.26 seconds\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"- In the last cell I called all LeNet-5 model's versions: Baseline, With Dropout, and with Batch normalization. This helps me compare their effectiveness against the baseline.","metadata":{}},{"cell_type":"markdown","source":"# Final result\n","metadata":{}},{"cell_type":"markdown","source":"- After running each model for 10 epochs, it is stated that batch normalization model has the highest testing accuracy among the all then comes with Dropput and Baseline model.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}